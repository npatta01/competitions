{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import lightgbm as lgb\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_modelfit_nocv(params, dtrain, dvalid, predictors, target='target', objective='binary', metrics='auc',\n",
    "                 feval=None, early_stopping_rounds=20, num_boost_round=3000, verbose_eval=10, categorical_features=None):\n",
    "    lgb_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': objective,\n",
    "        'metric':metrics,\n",
    "        'learning_rate': 0.08,\n",
    "        'num_leaves': 31,  # we should let it be smaller than 2^(max_depth)\n",
    "        'max_depth': -1,  # -1 means no limit\n",
    "        'min_child_samples': 20,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "        'max_bin': 255,  # Number of bucketed bin for feature values\n",
    "        'subsample': 0.6,  # Subsample ratio of the training instance.\n",
    "        'subsample_freq': 0,  # frequence of subsample, <=0 means no enable\n",
    "        'colsample_bytree': 0.3,  # Subsample ratio of columns when constructing each tree.\n",
    "        'min_child_weight': 5,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "        'subsample_for_bin': 200000,  # Number of samples for constructing bin\n",
    "        'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n",
    "        'reg_alpha': 0,  # L1 regularization term on weights\n",
    "        'reg_lambda': 0,  # L2 regularization term on weights\n",
    "        'nthread': 8,\n",
    "        'verbose': 0,\n",
    "        'metric':metrics\n",
    "    }\n",
    "\n",
    "    lgb_params.update(params)\n",
    "\n",
    "    print(\"preparing validation datasets\")\n",
    "\n",
    "    xgtrain = lgb.Dataset(dtrain[predictors].values, label=dtrain[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical_features\n",
    "                          )\n",
    "    xgvalid = lgb.Dataset(dvalid[predictors].values, label=dvalid[target].values,\n",
    "                          feature_name=predictors,\n",
    "                          categorical_feature=categorical_features\n",
    "                          )\n",
    "\n",
    "    evals_results = {}\n",
    "\n",
    "    bst1 = lgb.train(lgb_params, xgtrain, valid_sets=[xgtrain, xgvalid], valid_names=['train','valid'], evals_result=evals_results, num_boost_round=num_boost_round,\n",
    "                      early_stopping_rounds=early_stopping_rounds,\n",
    "                      verbose_eval=10, feval=feval)\n",
    "\n",
    "    n_estimators = bst1.best_iteration\n",
    "    print(\"\\nModel Report\")\n",
    "    print(\"n_estimators : \", n_estimators)\n",
    "    print(metrics+\":\", evals_results['valid'][metrics][n_estimators-1])\n",
    "\n",
    "    #ax = lgb.plot_importance(bst1,max_num_features=60)\n",
    "    #plt.show()\n",
    "\n",
    "    return bst1\n",
    "\n",
    "path = os.path.expanduser(\"~/data\")\n",
    "\n",
    "dtypes = {\n",
    "        'ip'            : 'uint32',\n",
    "        'app'           : 'uint16',\n",
    "        'device'        : 'uint16',\n",
    "        'os'            : 'uint16',\n",
    "        'channel'       : 'uint16',\n",
    "        'is_attributed' : 'uint8',\n",
    "        'click_id'      : 'uint32'\n",
    "        }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load train...\n",
      "load test...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py36/lib/python3.6/site-packages/pandas/core/frame.py:6211: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  sort=sort)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data prep...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('load train...')\n",
    "train_df = pd.read_csv(path+\"/train.csv\", dtype=dtypes, nrows=30000000, usecols=['ip','app','device','os', 'channel', 'click_time', 'is_attributed'])\n",
    "print('load test...')\n",
    "test_df = pd.read_csv(path+\"/test.csv\", dtype=dtypes, usecols=['ip','app','device','os', 'channel', 'click_time', 'click_id'])\n",
    "\n",
    "import gc\n",
    "\n",
    "len_train = len(train_df)\n",
    "train_df=train_df.append(test_df)\n",
    "\n",
    "del test_df\n",
    "gc.collect()\n",
    "\n",
    "print('data prep...')\n",
    "train_df['hour'] = pd.to_datetime(train_df.click_time).dt.hour.astype('uint8')\n",
    "del train_df['click_time']\n",
    "gc.collect()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 48790469 entries, 0 to 18790468\n",
      "Data columns (total 8 columns):\n",
      "app              uint16\n",
      "channel          uint16\n",
      "click_id         float64\n",
      "device           uint16\n",
      "ip               uint32\n",
      "is_attributed    float64\n",
      "os               uint16\n",
      "hour             uint8\n",
      "dtypes: float64(2), uint16(4), uint32(1), uint8(1)\n",
      "memory usage: 1.7 GB\n",
      "18790469\n",
      "2000000\n",
      "28000000\n"
     ]
    }
   ],
   "source": [
    "train_df.info()\n",
    "\n",
    "test_df = train_df[len_train:]\n",
    "print(len(test_df))\n",
    "val_df = train_df[(len_train-2000000):len_train]\n",
    "print(len(val_df))\n",
    "train_df = train_df[:(len_train-2000000)]\n",
    "print(len(train_df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "preparing validation datasets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py36/lib/python3.6/site-packages/lightgbm/basic.py:1158: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/anaconda/envs/py36/lib/python3.6/site-packages/lightgbm/basic.py:725: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 70 rounds.\n",
      "[10]\ttrain's auc: 0.939833\tvalid's auc: 0.927901\n",
      "[20]\ttrain's auc: 0.938698\tvalid's auc: 0.905554\n",
      "[30]\ttrain's auc: 0.946111\tvalid's auc: 0.914276\n",
      "[40]\ttrain's auc: 0.93461\tvalid's auc: 0.907831\n",
      "[50]\ttrain's auc: 0.944192\tvalid's auc: 0.909191\n",
      "[60]\ttrain's auc: 0.946299\tvalid's auc: 0.904565\n",
      "[70]\ttrain's auc: 0.954333\tvalid's auc: 0.907025\n",
      "Early stopping, best iteration is:\n",
      "[1]\ttrain's auc: 0.964569\tvalid's auc: 0.951834\n",
      "\n",
      "Model Report\n",
      "n_estimators :  1\n",
      "auc: 0.9518337846803466\n"
     ]
    }
   ],
   "source": [
    "target = 'is_attributed'\n",
    "predictors = ['app','device','os', 'channel', 'hour']\n",
    "categorical = ['app','device','os', 'channel', 'hour']\n",
    "\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['click_id'] = test_df['click_id'].astype('int')\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "print(\"Training...\")\n",
    "params = {\n",
    "    'learning_rate': 0.2,\n",
    "    'num_leaves': 1400,  # we should let it be smaller than 2^(max_depth)\n",
    "    'max_depth': -1,  # -1 means no limit\n",
    "    'min_child_samples': 100,  # Minimum number of data need in a child(min_data_in_leaf)\n",
    "    'max_bin': 100,  # Number of bucketed bin for feature values\n",
    "    'subsample': .9,  # Subsample ratio of the training instance.\n",
    "    'subsample_freq': 1,  # frequence of subsample, <=0 means no enable\n",
    "    'colsample_bytree': 0.9,  # Subsample ratio of columns when constructing each tree.\n",
    "    'min_child_weight': 0,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n",
    "    'scale_pos_weight':90\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bst = lgb_modelfit_nocv(params, train_df, val_df, predictors, target, objective='binary', metrics='auc',\n",
    "                        early_stopping_rounds=70, verbose_eval=True, num_boost_round=1000, categorical_features=categorical)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting...\n"
     ]
    }
   ],
   "source": [
    "del train_df\n",
    "del val_df\n",
    "gc.collect()\n",
    "\n",
    "print(\"Predicting...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing...\n",
      "done...\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 18790469 entries, 0 to 18790468\n",
      "Data columns (total 2 columns):\n",
      "click_id         int64\n",
      "is_attributed    float64\n",
      "dtypes: float64(1), int64(1)\n",
      "memory usage: 430.1 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "sub['is_attributed'] = bst.predict(test_df[predictors])\n",
    "print(\"writing...\")\n",
    "sub.to_csv('lgb_sub_tint.csv',index=False)\n",
    "print(\"done...\")\n",
    "print(sub.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
